{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df885f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datasetTrain = pd.read_csv('Corona_NLP_train.csv',sep=',', encoding='latin1')\n",
    "datasetTest = pd.read_csv('Corona_NLP_test.csv',sep=',', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b40cc904",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = datasetTrain[['OriginalTweet']]\n",
    "X_test  = datasetTest[['OriginalTweet']]\n",
    "y_train = datasetTrain[['Sentiment']]\n",
    "y_test  = datasetTest[['Sentiment']]\n",
    "\n",
    "#foreign languages\n",
    "X_train = X_train.drop([1638, 8729])\n",
    "y_train = y_train.drop([1638, 8729])\n",
    "X_test = X_test.drop([345])\n",
    "y_test = y_test.drop([345])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42fb586b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Extremely Negative', 'Extremely Positive', 'Negative', 'Neutral', 'Positive')\n",
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(y_train)\n",
    "\n",
    "y_train = lb.transform(y_train)\n",
    "y_test = lb.transform(y_test)\n",
    "\n",
    "y_map = dict(zip(lb.classes_, lb.transform(lb.classes_)))\n",
    "\n",
    "y_keys, y_values = zip(*y_map.items())\n",
    "print(y_keys)\n",
    "y_values = np.argmax(y_values, axis=1)\n",
    "print(y_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f61fdb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values.tolist()\n",
    "X_test  = X_test.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "957a6bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean(text):\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace(artifact1, \"'\")\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace(artifact2, ' \" ')\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace(artifact3, ' \" ')\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace(artifact4, '.')\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace(artifact5, ',')\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace(artifact6, \"'\")\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace(artifact7, \",\")\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace(artifact8, \"\")\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace(artifact9, \"\")\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace(artifact10, \"\")\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace(artifact11, \"\")\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace(artifact12, \"\")\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace(artifact13, \"\")\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace(artifact14, \"\")\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace(artifact15, '\"')\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace(artifact16, '')\n",
    "    for i in range(len(text)):\n",
    "        a_string = text[i]\n",
    "        text[i] = re.sub('http[s]?://\\S+', ' ', a_string, flags=re.MULTILINE)\n",
    "    for i in range(len(text)):\n",
    "        a_string = text[i]\n",
    "        text[i] = re.sub('#', '# ', a_string, flags=re.MULTILINE)\n",
    "    for i in range(len(text)):\n",
    "        a_string = text[i]\n",
    "        text[i] = re.sub('@\\w+', ' ', a_string, flags=re.MULTILINE)\n",
    "    for i in range(len(text)):\n",
    "        a_string = text[i]\n",
    "        text[i] = re.sub('(,)', ' ,', a_string, flags=re.MULTILINE)\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace('. ', ' . ')\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace('. \\n', ' . ')\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace('...', ' ... ')\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace('!', ' !')\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace(':', ' :')\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace('?', ' ?')\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace('\"',  ' \" ')\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace('&amp;', '&') \n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace('^\\s', '')\n",
    "    for i in range(len(text)):\n",
    "        a_string = text[i]\n",
    "        text[i] = re.sub('\\.\\n', ' . ', a_string, flags=re.MULTILINE)\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace('.', ' . ')\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace(')', ' ) ')\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace('(', ' ( ')\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace('.  .  .', '...')\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].lower()\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace(\"'s\", \" 's\")\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace(\"Ã¢\", \"\")\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace(\"*\", \"\")\n",
    "    for i in range(len(text)):\n",
    "        text[i] = \" \".join(text[i].split())    \n",
    "    \n",
    "    return text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa85ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(text):\n",
    "    new_text = []\n",
    "    for item in text:\n",
    "        for i in item:\n",
    "            new_text.append(i)\n",
    "    return new_text\n",
    "\n",
    "def String_to_list_of_string(text):\n",
    "    for i in range(len(text)):\n",
    "        text[i] = list(text[i].split(\" \"))   \n",
    "    return text\n",
    "\n",
    "def truncate_and_pad(text):\n",
    "    sentence_length = 120\n",
    "    pad_token = '<pad>'\n",
    "    for i in range(len(text)):\n",
    "        pad_amount = sentence_length - len(text[i])\n",
    "        pad_list = [pad_token] * pad_amount\n",
    "        text[i] = text[i] + pad_list\n",
    "        text[i] = text[i][:120]\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_ypred(y_pred, y_test):\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    uu = accuracy_score(y_test, y_pred)\n",
    "    print('accuracy score on X_test is :', uu)\n",
    "    zz = confusion_matrix(y_test, y_pred, labels=y_values)\n",
    "    zz = pd.DataFrame(zz)\n",
    "    zz = zz.set_axis(list(y_keys), axis='columns', inplace=False)\n",
    "    zz = zz.set_axis(list(y_keys), axis='index', inplace=False)\n",
    "    print(zz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a22824e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = flatten(X_train)\n",
    "X_test  = flatten(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3630fed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "art1 = X_train[5][21]\n",
    "art2 = X_train[5][22]\n",
    "artifact1 = art1+art2  \n",
    "\n",
    "art1 = X_train[18][62]\n",
    "art2 = X_train[18][63]\n",
    "artifact2 = art1+art2\n",
    "\n",
    "art1 = X_train[18][163]\n",
    "art2 = X_train[18][164]\n",
    "artifact3 = art1+art2\n",
    "\n",
    "art1 = X_train[118][40]\n",
    "art2 = X_train[118][41]\n",
    "artifact4 = art1+art2\n",
    "\n",
    "art1 = X_train[136][91]\n",
    "art2 = X_train[136][92]\n",
    "artifact5 = art1+art2\n",
    "\n",
    "art1 = X_train[152][27]\n",
    "art2 = X_train[152][28]\n",
    "artifact6 = art1+art2\n",
    "\n",
    "art1 = X_train[280][65]\n",
    "art2 = X_train[280][66]\n",
    "artifact7 = art1+art2\n",
    "\n",
    "art1 = X_train[459][157]\n",
    "art2 = X_train[459][158]\n",
    "artifact8 = art1+art2\n",
    "\n",
    "art1 = X_train[2131][55]\n",
    "art2 = X_train[2131][56]\n",
    "artifact9 = art1+art2\n",
    "\n",
    "art1 = X_train[2460][11]\n",
    "art2 = X_train[2460][12]\n",
    "artifact10 = art1+art2\n",
    "\n",
    "art1 = X_train[2928][176]\n",
    "art2 = X_train[2928][177]\n",
    "artifact11 = art1+art2\n",
    "\n",
    "art1 = X_train[5555][258]\n",
    "art2 = X_train[5555][259]\n",
    "artifact12 = art1+art2\n",
    "\n",
    "art1 = X_train[19806][0]\n",
    "art2 = X_train[19806][1]\n",
    "artifact13 = art1+art2\n",
    "\n",
    "art1 = X_train[24130][237]\n",
    "art2 = X_train[24130][238]\n",
    "artifact14 = art1+art2\n",
    "\n",
    "art1 = X_train[30221][0]\n",
    "art2 = X_train[30221][1]\n",
    "artifact15 = art1+art2\n",
    "\n",
    "art1 = X_train[39811][185]\n",
    "art2 = X_train[39811][186]\n",
    "artifact16 = art1+art2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae2f5537",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = clean(X_train)\n",
    "X_test  = clean(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85475e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('your_file1.txt', 'w') as f:\n",
    "#     for item in X_train:\n",
    "#         f.write(\"%s\\n\" % item)\n",
    "\n",
    "# with open('your_file2.txt', 'w') as f:\n",
    "#     for item in X_test:\n",
    "#         f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95b3ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# cv = CountVectorizer()\n",
    "# cv.fit(X_train)\n",
    "\n",
    "# X_train_sparse = cv.transform(X_train)\n",
    "# X_test_sparse  = cv.transform(X_test)\n",
    "\n",
    "# dict_sparse = cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0de6f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = String_to_list_of_string(X_train)\n",
    "X_test  = String_to_list_of_string(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56ff2fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = truncate_and_pad(X_train)\n",
    "X_test  = truncate_and_pad(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a66856b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'and', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7edf0fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "print(max(len(x) for x in X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cabb443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_dict(x_dict, n_words):\n",
    "    n_words = n_words+1\n",
    "    z = x_dict\n",
    "    z = flatten(z)\n",
    "    z = Counter(z)\n",
    "    z = z.most_common(n_words)\n",
    "    z_list = [x[0] for x in z]\n",
    "    z_list.append('<unk>')\n",
    "    return z_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d57c01ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dict = get_dict(X_train, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21609e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def keys_values(text):\n",
    "#     keys = []\n",
    "#     values = []\n",
    "#     value = -1\n",
    "#     for item in text:\n",
    "#         for i in item:\n",
    "#             if i not in keys:\n",
    "#                 keys.append(i)\n",
    "#                 value = value + 1\n",
    "#                 values.append(value)\n",
    "    \n",
    "#     keys.append('<unk>')\n",
    "#     unk_value = value + 1\n",
    "#     values.append(unk_value)\n",
    "#     return keys, values\n",
    "\n",
    "# X_keys, X_values = keys_values(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1244d287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_seq(text, x_dict):\n",
    "    new_text = []\n",
    "    unk_token = x_dict.index('<unk>')\n",
    "    for sequence in text:\n",
    "        list = []\n",
    "        for word in sequence:\n",
    "            if word in x_dict:\n",
    "                index = x_dict.index(word)\n",
    "                list.append(index)\n",
    "            else:\n",
    "                list.append(unk_token)\n",
    "        new_text.append(list)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3036bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_coded = code_seq(X_train, X_dict)\n",
    "X_test_coded = code_seq(X_test, X_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d130876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_coded = np.array(X_train_coded)\n",
    "X_test_coded  = np.array(X_test_coded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "101709f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import model packages\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GlobalMaxPool1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ff3ebcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model\n",
    "def get_model(n_inputs, n_outputs, n_dict):\n",
    "    \n",
    "    print(n_inputs)\n",
    "    print(n_outputs)\n",
    "    print(n_dict)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(n_dict, 64, input_length=n_inputs))\n",
    "    model.add(LSTM(128, activation='relu', return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64))\n",
    "    \n",
    "#     model=Sequential()\n",
    "#     model.add(Embedding(n_dict,64,input_length=n_inputs))\n",
    "#     model.add(LSTM(64,return_sequences=True))\n",
    "#     model.add(GlobalMaxPool1D())\n",
    "#     model.add(Dense(64))\n",
    "    \n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    \n",
    "    #opt = SGD(lr=0.01, momentum=0.9)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam' , metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#train the model\n",
    "def evaluate_model(X_train, y_train, X_test, y_test, x_dict, n_epochs):\n",
    "    results = list()\n",
    "    #n_inputs, n_outputs = X_train.shape[1], y_train.shape[1]\n",
    "    #call the previously defined model\n",
    "    n_inputs  = X_train.shape[1]\n",
    "    n_outputs = y_train.shape[1]\n",
    "    n_dict = len(x_dict)+1\n",
    "    model = get_model(n_inputs, n_outputs, n_dict)\n",
    "    #train the model\n",
    "    print('Train:')\n",
    "    model.fit(X_train, y_train, verbose=1, epochs=n_epochs)\n",
    "    #test the model, return a confusion matrix\n",
    "    print('Evaluate:')\n",
    "    y_pred = model.predict_classes(X_test, verbose=1)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fff3cdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "5\n",
      "2003\n",
      "Train:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antoi\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "41155/41155 [==============================] - 170s 4ms/step - loss: 1.0665 - accuracy: 0.5833\n",
      "Epoch 2/10\n",
      "41155/41155 [==============================] - 164s 4ms/step - loss: 0.8714 - accuracy: 0.6814\n",
      "Epoch 3/10\n",
      "41155/41155 [==============================] - 164s 4ms/step - loss: 0.8059 - accuracy: 0.7029\n",
      "Epoch 4/10\n",
      "41155/41155 [==============================] - 156s 4ms/step - loss: 0.7429 - accuracy: 0.7227\n",
      "Epoch 5/10\n",
      "41155/41155 [==============================] - 137s 3ms/step - loss: 0.6741 - accuracy: 0.74741s - l\n",
      "Epoch 6/10\n",
      "41155/41155 [==============================] - 132s 3ms/step - loss: 0.6036 - accuracy: 0.7724\n",
      "Epoch 7/10\n",
      "41155/41155 [==============================] - 134s 3ms/step - loss: 0.5289 - accuracy: 0.7997\n",
      "Epoch 8/10\n",
      "41155/41155 [==============================] - 145s 4ms/step - loss: 0.4492 - accuracy: 0.8333\n",
      "Epoch 9/10\n",
      "41155/41155 [==============================] - 163s 4ms/step - loss: 0.3907 - accuracy: 0.8590\n",
      "Epoch 10/10\n",
      "41155/41155 [==============================] - 170s 4ms/step - loss: 0.3076 - accuracy: 0.8888\n",
      "Evaluate:\n",
      "3797/3797 [==============================] - 4s 946us/step\n"
     ]
    }
   ],
   "source": [
    "results_coded = evaluate_model(X_train_coded, y_train, X_test_coded, y_test, X_dict, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "af3362c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score on X_test is : 0.5791414274427179\n",
      "                    Extremely Negative  Extremely Positive  Negative  Neutral  \\\n",
      "Extremely Negative                 355                   7       184       22   \n",
      "Extremely Positive                  14                 379        42       19   \n",
      "Negative                           199                  23       522      146   \n",
      "Neutral                             29                  11        71      441   \n",
      "Positive                            49                 142       147      107   \n",
      "\n",
      "                    Positive  \n",
      "Extremely Negative        24  \n",
      "Extremely Positive       145  \n",
      "Negative                 151  \n",
      "Neutral                   66  \n",
      "Positive                 502  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "evaluate_ypred(results_coded, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a50b737c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 4 ... 3 4 4]\n"
     ]
    }
   ],
   "source": [
    "print(results_coded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc4ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
